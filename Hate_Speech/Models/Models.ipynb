{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8364bd35",
   "metadata": {
    "id": "8364bd35"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import math\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.layers import Input,Reshape,Conv2D,MaxPool2D,Concatenate,Flatten\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix,precision_score,recall_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout, Bidirectional, GlobalAveragePooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import regularizers\n",
    "# from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a586ebb9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "a586ebb9",
    "outputId": "fc6c122e-987e-4e4d-d7b1-f4a45d5951a8",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>id</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"@KeyshawnSwag: Lmfao this cat started beating...</td>\n",
       "      <td>0</td>\n",
       "      <td>keyshawnswag lmfao this cat started beating th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @digaveliavelife: Lol I be eatin da shit ou...</td>\n",
       "      <td>1</td>\n",
       "      <td>rt digaveliavelife lol i be eatin da shit outt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @QueenReenie_: How bitch how? &amp;#8220;@_Vont...</td>\n",
       "      <td>2</td>\n",
       "      <td>rt queenreenie how bitch how 8220vontethekidd ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet  id  \\\n",
       "0      1  \"@KeyshawnSwag: Lmfao this cat started beating...   0   \n",
       "1      1  RT @digaveliavelife: Lol I be eatin da shit ou...   1   \n",
       "2      1  RT @QueenReenie_: How bitch how? &#8220;@_Vont...   2   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  keyshawnswag lmfao this cat started beating th...  \n",
       "1  rt digaveliavelife lol i be eatin da shit outt...  \n",
       "2  rt queenreenie how bitch how 8220vontethekidd ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('C:/Users/HP/Downloads/bert_preprocessed_train.csv')\n",
    "df = df.drop(['Unnamed: 0'], axis = 1)\n",
    "df.head(3)\n",
    "# df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e984b5b6",
   "metadata": {
    "id": "e984b5b6"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"C:/Users/HP/Downloads/bert_preprocessed_test.csv\") #,encoding='unicode_escape'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Zac9MWUJqXxY",
   "metadata": {
    "id": "Zac9MWUJqXxY"
   },
   "outputs": [],
   "source": [
    "# Renaming the columns \n",
    "df.rename(columns = {'tweet':'original_tweet'}, inplace = True)\n",
    "df.rename(columns = {'cleaned_text':'tweet'}, inplace = True)\n",
    "\n",
    "test.rename(columns = {'tweet':'original_tweet'}, inplace = True)\n",
    "test.rename(columns = {'cleaned_text':'tweet'}, inplace = True)\n",
    "\n",
    "# df.drop('Unnamed: 0', inplace=True, axis=1)\n",
    "test.drop('Unnamed: 0', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "WEgRTEwMqqLg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "WEgRTEwMqqLg",
    "outputId": "300bb84e-fd91-4f61-8a79-093a553cbbbf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>original_tweet</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"@KeyshawnSwag: Lmfao this cat started beating...</td>\n",
       "      <td>0</td>\n",
       "      <td>keyshawnswag lmfao this cat started beating th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @digaveliavelife: Lol I be eatin da shit ou...</td>\n",
       "      <td>1</td>\n",
       "      <td>rt digaveliavelife lol i be eatin da shit outt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @QueenReenie_: How bitch how? &amp;#8220;@_Vont...</td>\n",
       "      <td>2</td>\n",
       "      <td>rt queenreenie how bitch how 8220vontethekidd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Floppy bird chalmers over here taking after hi...</td>\n",
       "      <td>3</td>\n",
       "      <td>floppy bird chalmers over here taking after hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Boolin' in the bando wit a few bitches\"</td>\n",
       "      <td>4</td>\n",
       "      <td>boolin in the bando wit a few bitches</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                     original_tweet  id  \\\n",
       "0      1  \"@KeyshawnSwag: Lmfao this cat started beating...   0   \n",
       "1      1  RT @digaveliavelife: Lol I be eatin da shit ou...   1   \n",
       "2      1  RT @QueenReenie_: How bitch how? &#8220;@_Vont...   2   \n",
       "3      2  Floppy bird chalmers over here taking after hi...   3   \n",
       "4      1           \"Boolin' in the bando wit a few bitches\"   4   \n",
       "\n",
       "                                               tweet  \n",
       "0  keyshawnswag lmfao this cat started beating th...  \n",
       "1  rt digaveliavelife lol i be eatin da shit outt...  \n",
       "2  rt queenreenie how bitch how 8220vontethekidd ...  \n",
       "3  floppy bird chalmers over here taking after hi...  \n",
       "4              boolin in the bando wit a few bitches  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "XP80yRmrwOeH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "XP80yRmrwOeH",
    "outputId": "9e676c4f-a1e7-40b6-e116-a278e5529b27"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_tweet</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @DefendWallSt: Tell me how you really feel....</td>\n",
       "      <td>0</td>\n",
       "      <td>rt defendwallst tell me how you really feel rt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&amp;#8220;@spannerhead29: @VertrellCarter #faggot...</td>\n",
       "      <td>1</td>\n",
       "      <td>8220spannerhead29 vertrellcarter faggot8221 re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ross text me back you twat</td>\n",
       "      <td>2</td>\n",
       "      <td>ross text me back you twat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Your name follows the word cunt in the diction...</td>\n",
       "      <td>3</td>\n",
       "      <td>your name follows the word cunt in the diction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@brendacoyt_3 : ahah chill my bitch look bette...</td>\n",
       "      <td>4</td>\n",
       "      <td>brendacoyt3  ahah chill my bitch look better 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      original_tweet  id  \\\n",
       "0  RT @DefendWallSt: Tell me how you really feel....   0   \n",
       "1  &#8220;@spannerhead29: @VertrellCarter #faggot...   1   \n",
       "2                         Ross text me back you twat   2   \n",
       "3  Your name follows the word cunt in the diction...   3   \n",
       "4  @brendacoyt_3 : ahah chill my bitch look bette...   4   \n",
       "\n",
       "                                               tweet  \n",
       "0  rt defendwallst tell me how you really feel rt...  \n",
       "1  8220spannerhead29 vertrellcarter faggot8221 re...  \n",
       "2                         ross text me back you twat  \n",
       "3  your name follows the word cunt in the diction...  \n",
       "4  brendacoyt3  ahah chill my bitch look better 1...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9aeb3e60",
   "metadata": {
    "id": "9aeb3e60"
   },
   "outputs": [],
   "source": [
    "# def clean_str(in_str):\n",
    "#     in_str = str(in_str)\n",
    "#     # replace urls with 'url'\n",
    "#     in_str = re.sub(r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})\", \"url\", in_str)\n",
    "#     in_str = re.sub(r'([^\\s\\w]|_)+', '', in_str)\n",
    "#     return in_str.strip().lower()\n",
    "\n",
    "\n",
    "# df['tweet'] = df['tweet'].apply(clean_str)\n",
    "# test['tweet'] = test['tweet'].apply(clean_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a13a85b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0a13a85b",
    "outputId": "faf9a059-89f9-4bf4-b223-34a947b442ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    15398\n",
       "2     3291\n",
       "0     1137\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c8ee39c",
   "metadata": {
    "id": "4c8ee39c"
   },
   "outputs": [],
   "source": [
    "df_0 = df[df['label'] == 0].sample(frac=1)\n",
    "df_1 = df[df['label'] == 1].sample(frac=1)\n",
    "df_2 = df[df['label'] == 2].sample(frac=1)\n",
    "sample_size = 1137\n",
    "data = pd.concat([df_0.head(sample_size), df_1.head(sample_size), df_2.head(sample_size)]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d8b557b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d8b557b",
    "outputId": "32249573-bd8c-49a5-9e29-e7e43a5ba6d4"
   },
   "outputs": [],
   "source": [
    "# data['l'] = data['tweet'].apply(lambda x: len(str(x).split(' ')))\n",
    "# print(\"mean length of sentence: \" + str(data.l.mean()))\n",
    "# print(\"max length of sentence: \" + str(data.l.max()))\n",
    "# print(\"std dev length of sentence: \" + str(data.l.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4901a9",
   "metadata": {
    "id": "7d4901a9"
   },
   "source": [
    "### Logistic Regression with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d870c750",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d870c750",
    "outputId": "74ff9e30-d5b3-4afe-e1fa-0b62bf97e929"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7726465364120781\n"
     ]
    }
   ],
   "source": [
    "review = data['tweet'].values\n",
    "label = data['label'].values\n",
    "review_train, review_test, label_train, label_test = train_test_split(review, label, test_size=0.33, random_state=42)\n",
    "\n",
    "review_vectorizer = CountVectorizer()\n",
    "review_vectorizer.fit(review_train)\n",
    "Xlr_train = review_vectorizer.transform(review_train)\n",
    "Xlr_test  = review_vectorizer.transform(review_test)\n",
    "\n",
    "LRmodel = LogisticRegression()\n",
    "LRmodel.fit(Xlr_train, label_train)\n",
    "score = LRmodel.score(Xlr_test, label_test)\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2767df5",
   "metadata": {
    "id": "f2767df5"
   },
   "source": [
    "### Logistic Regression with Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4532fd58",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4532fd58",
    "outputId": "3a51484b-c74a-48b0-ae5b-c6db44671018"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy =  0.931792516750734\n",
      "Test score =  0.8893473941616995\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "features = tfidf.fit_transform(df.tweet).toarray()\n",
    "labels = df.label\n",
    "\n",
    "review_train, review_test, label_train, label_test = train_test_split(features, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "LRmodel = LogisticRegression()\n",
    "LRmodel.fit(review_train, label_train)\n",
    "\n",
    "train_score = LRmodel.score(review_train, label_train)\n",
    "test_score = LRmodel.score(review_test, label_test)\n",
    "\n",
    "print(\"Training Accuracy = \", train_score)\n",
    "print(\"Test score = \", test_score)\n",
    "# submission=test\n",
    "# submission['label']=pred\n",
    "# submission.to_csv(\"submission.csv\", index = False)\n",
    "# submission.to_csv(\"D:/res5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97YX73Zw2Kfv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97YX73Zw2Kfv",
    "outputId": "f3dc1ac9-75cd-4170-f9ec-dc8ab0cef3f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Generating submission file for the above cell\n",
    "LRmodel = LogisticRegression()\n",
    "LRmodel.fit(features, labels)\n",
    "\n",
    "test_features = tfidf.transform(test.tweet).toarray()\n",
    "pred = LRmodel.predict(test_features)\n",
    "\n",
    "submission=test\n",
    "submission['label']=pred\n",
    "submission.to_csv(\"LR_TDFIF_CleanedData_submission.csv\", index = False)\n",
    "submission.to_csv(\"LR_CleanData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36463c6b",
   "metadata": {
    "id": "c94ae23a"
   },
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d61c7d0d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d61c7d0d",
    "outputId": "93e97a07-c63d-4fb0-c71f-daffd9c5fa8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.01      0.02       366\n",
      "           1       0.84      0.99      0.91      5083\n",
      "           2       0.91      0.44      0.59      1094\n",
      "\n",
      "    accuracy                           0.84      6543\n",
      "   macro avg       0.78      0.48      0.51      6543\n",
      "weighted avg       0.84      0.84      0.81      6543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(review_train, label_train)  #model\n",
    "y_predict = nb_tfidf.predict(review_test)\n",
    "print(classification_report(label_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88b85de9",
   "metadata": {
    "id": "88b85de9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Best parameters for NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f50ede3c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f50ede3c",
    "outputId": "e2e01ca2-5310-42d1-9f08-96258ae2441e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best Accuracy Through Grid Search : 0.860\n",
      "Best Parameters :  {'alpha': 0.1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.11      0.18       366\n",
      "           1       0.87      0.97      0.92      5083\n",
      "           2       0.84      0.60      0.70      1094\n",
      "\n",
      "    accuracy                           0.86      6543\n",
      "   macro avg       0.75      0.56      0.60      6543\n",
      "weighted avg       0.85      0.86      0.84      6543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With tdfif vectorizer\n",
    "params = {'alpha': [0.01,0.1,0.5,1,10],\n",
    "         }\n",
    "\n",
    "multinomial_nb_grid = GridSearchCV(MultinomialNB(), param_grid=params, n_jobs=-1, cv=5, verbose=5)\n",
    "multinomial_nb_grid.fit(review_train, label_train)\n",
    "y_predict = multinomial_nb_grid.predict(review_test)\n",
    "print('Best Accuracy Through Grid Search : %.3f'%multinomial_nb_grid.best_score_)\n",
    "print('Best Parameters : ',multinomial_nb_grid.best_params_)\n",
    "print(classification_report(label_test,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776c9855",
   "metadata": {
    "id": "1b11bae1"
   },
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84b1c7bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84b1c7bc",
    "outputId": "c31372f4-323f-485e-c506-e132dc71900a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.27      0.29       366\n",
      "           1       0.93      0.92      0.92      5083\n",
      "           2       0.78      0.83      0.80      1094\n",
      "\n",
      "    accuracy                           0.87      6543\n",
      "   macro avg       0.67      0.67      0.67      6543\n",
      "weighted avg       0.87      0.87      0.87      6543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With tdfif vectorizer\n",
    "nb_tfidf = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "nb_tfidf.fit(review_train, label_train)  #model\n",
    "y_predict = nb_tfidf.predict(review_test)\n",
    "print(classification_report(label_test,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34b5ae2",
   "metadata": {
    "id": "9e9acf1d"
   },
   "source": [
    " ### Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91eeda51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91eeda51",
    "outputId": "c651a477-8a4d-45f8-f124-73280b531b64",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       366\n",
      "           1       0.78      1.00      0.87      5083\n",
      "           2       0.00      0.00      0.00      1094\n",
      "\n",
      "    accuracy                           0.78      6543\n",
      "   macro avg       0.26      0.33      0.29      6543\n",
      "weighted avg       0.60      0.78      0.68      6543\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# I am not running this\n",
    "nb_tfidf = svm.SVC(kernel='rbf', C=1,gamma='auto')\n",
    "nb_tfidf.fit(review_train, label_train)  #modelb\n",
    "y_predict = nb_tfidf.predict(review_test)\n",
    "print(classification_report(label_test,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8433853",
   "metadata": {
    "id": "b39aee68"
   },
   "source": [
    "### Support Vector Classification with tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c75f17ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c75f17ff",
    "outputId": "323a8dac-36d4-4a70-d265-0fecabef9bcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy =  0.9634871640442672\n",
      "Test score =  0.8820113097967294\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "features = tfidf.fit_transform(df.tweet).toarray()\n",
    "labels = df.label\n",
    "\n",
    "review_train, review_test, label_train, label_test = train_test_split(features, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "svc_model = LinearSVC()\n",
    "svc_model.fit(review_train, label_train)\n",
    "\n",
    "train_score = svc_model.score(review_train, label_train)\n",
    "test_score = svc_model.score(review_test, label_test)\n",
    "\n",
    "print(\"Training Accuracy = \", train_score)\n",
    "print(\"Test score = \", test_score)\n",
    "\n",
    "# submission=test\n",
    "# submission['label']=pred\n",
    "# submission.drop(['', 'Phrase'], axis=1, inplace=True)\n",
    "# submission.to_csv(\"submission.csv\", index = False)\n",
    "# submission.to_csv(\"D:/res3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "V_c3NgfrLkum",
   "metadata": {
    "id": "V_c3NgfrLkum"
   },
   "outputs": [],
   "source": [
    "# Generating submission file for the above cell\n",
    "svc_model = LinearSVC()\n",
    "svc_model.fit(features, labels)\n",
    "\n",
    "test_features = tfidf.transform(test.tweet).toarray()\n",
    "pred = svc_model.predict(test_features)\n",
    "\n",
    "submission=test\n",
    "submission['label']=pred\n",
    "submission.to_csv(\"SVC_TDFIF_CleanedData_submission.csv\", index = False)\n",
    "submission.to_csv(\"SVC_CleanData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d4f45",
   "metadata": {
    "id": "53550131"
   },
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d194fea4",
   "metadata": {
    "id": "d194fea4",
    "outputId": "d66c5117-a81d-466f-f4d7-a94295d6e7ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.68      0.68       277\n",
      "           1       0.76      0.73      0.74       301\n",
      "           2       0.83      0.85      0.84       275\n",
      "\n",
      "    accuracy                           0.75       853\n",
      "   macro avg       0.75      0.76      0.76       853\n",
      "weighted avg       0.75      0.75      0.75       853\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gboost_m = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)\n",
    "train_pred = gboost_m.fit(Xlr_train, label_train).predict(Xlr_test)\n",
    "print(classification_report(label_test,y_predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe6c177",
   "metadata": {
    "id": "0fe6c177"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8f878bc",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b139957f",
   "metadata": {
    "id": "b139957f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set size 1983\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 32\n",
    "max_features = 20000 # this is the number of words we care about\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>')\n",
    "tokenizer.fit_on_texts(df['tweet'].values)\n",
    "\n",
    "# this takes our sentences and replaces each word with an integer\n",
    "X = tokenizer.texts_to_sequences(df['tweet'].values)\n",
    "x_test = tokenizer.texts_to_sequences(test['tweet'].values)\n",
    "\n",
    "# we then pad the sequences so they're all the same length (sequence_length)\n",
    "X = pad_sequences(X, sequence_length)\n",
    "x_test = pad_sequences(x_test, sequence_length)\n",
    "y = pd.get_dummies(df['label']).values\n",
    "\n",
    "# where there isn't a test set, Kim keeps back 10% of the data for testing, I'm going to do the same since we have an ok amount to play with\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "print(\"test set size \" + str(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775aba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5de41965",
   "metadata": {
    "id": "0db33b69"
   },
   "source": [
    "### Base Model (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "909941ac",
   "metadata": {
    "id": "909941ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32538 unique tokens.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 32, 128)           2560128   \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 1032      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 27        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,561,187\n",
      "Trainable params: 2,561,187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "447/447 [==============================] - 11s 24ms/step - loss: 0.6153 - accuracy: 0.7821 - val_loss: 0.5019 - val_accuracy: 0.7938\n",
      "Epoch 2/20\n",
      "447/447 [==============================] - 10s 23ms/step - loss: 0.3625 - accuracy: 0.8753 - val_loss: 0.3644 - val_accuracy: 0.8748\n",
      "Epoch 3/20\n",
      "447/447 [==============================] - 10s 23ms/step - loss: 0.2503 - accuracy: 0.9156 - val_loss: 0.3231 - val_accuracy: 0.8857\n",
      "Epoch 4/20\n",
      "447/447 [==============================] - 10s 23ms/step - loss: 0.1754 - accuracy: 0.9421 - val_loss: 0.3147 - val_accuracy: 0.8913\n",
      "Epoch 5/20\n",
      "447/447 [==============================] - 11s 24ms/step - loss: 0.1219 - accuracy: 0.9620 - val_loss: 0.3269 - val_accuracy: 0.8882\n",
      "Epoch 6/20\n",
      "447/447 [==============================] - 11s 24ms/step - loss: 0.0880 - accuracy: 0.9716 - val_loss: 0.3733 - val_accuracy: 0.8823\n",
      "Epoch 7/20\n",
      "447/447 [==============================] - 10s 23ms/step - loss: 0.0655 - accuracy: 0.9813 - val_loss: 0.3815 - val_accuracy: 0.8907\n",
      "Epoch 8/20\n",
      "447/447 [==============================] - 10s 23ms/step - loss: 0.0497 - accuracy: 0.9857 - val_loss: 0.4154 - val_accuracy: 0.8806\n",
      "Epoch 9/20\n",
      "447/447 [==============================] - 10s 23ms/step - loss: 0.0390 - accuracy: 0.9888 - val_loss: 0.4628 - val_accuracy: 0.8784\n",
      "Epoch 10/20\n",
      "447/447 [==============================] - 10s 23ms/step - loss: 0.0321 - accuracy: 0.9898 - val_loss: 0.4932 - val_accuracy: 0.8846\n",
      "Epoch 11/20\n",
      "447/447 [==============================] - 10s 23ms/step - loss: 0.0267 - accuracy: 0.9926 - val_loss: 0.5188 - val_accuracy: 0.8837\n",
      "Epoch 12/20\n",
      "447/447 [==============================] - 11s 24ms/step - loss: 0.0233 - accuracy: 0.9934 - val_loss: 0.5524 - val_accuracy: 0.8739\n",
      "Epoch 13/20\n",
      "447/447 [==============================] - 10s 23ms/step - loss: 0.0202 - accuracy: 0.9947 - val_loss: 0.6109 - val_accuracy: 0.8734\n",
      "Epoch 14/20\n",
      "447/447 [==============================] - 10s 22ms/step - loss: 0.0184 - accuracy: 0.9945 - val_loss: 0.6317 - val_accuracy: 0.8778\n",
      "Epoch 15/20\n",
      "447/447 [==============================] - 10s 23ms/step - loss: 0.0154 - accuracy: 0.9953 - val_loss: 0.6696 - val_accuracy: 0.8641\n",
      "Epoch 16/20\n",
      "447/447 [==============================] - 10s 23ms/step - loss: 0.0133 - accuracy: 0.9962 - val_loss: 0.6940 - val_accuracy: 0.8672\n",
      "Epoch 17/20\n",
      "447/447 [==============================] - 10s 23ms/step - loss: 0.0126 - accuracy: 0.9965 - val_loss: 0.7208 - val_accuracy: 0.8686\n",
      "Epoch 18/20\n",
      "447/447 [==============================] - 10s 23ms/step - loss: 0.0123 - accuracy: 0.9964 - val_loss: 0.7673 - val_accuracy: 0.8635\n",
      "Epoch 19/20\n",
      "447/447 [==============================] - 10s 23ms/step - loss: 0.0112 - accuracy: 0.9968 - val_loss: 0.8005 - val_accuracy: 0.8627\n",
      "Epoch 20/20\n",
      "447/447 [==============================] - 11s 24ms/step - loss: 0.0098 - accuracy: 0.9971 - val_loss: 0.8205 - val_accuracy: 0.8607\n",
      "62/62 [==============================] - 0s 869us/step\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.8978 - accuracy: 0.8477\n",
      "Test: accuracy = 0.847705  ;  loss = 0.897752\n",
      "Precision :  0.6544494352789866\n",
      "Recall :  0.6277673350041771\n",
      "F1-Score :  0.6401274523643211\n"
     ]
    }
   ],
   "source": [
    "# Building the BASELINE MODEL\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "embedding_dim = 300 # Kim uses 300 here\n",
    "num_filters = 100\n",
    "num_words = min(max_features, len(word_index)) + 1\n",
    "regularise = tf.keras.regularizers.l2(0.001)\n",
    "\n",
    "base_model = Sequential()\n",
    "base_model.add(Embedding(num_words,128,input_length=X_train.shape[1]))\n",
    "base_model.add(GlobalAveragePooling1D())\n",
    "base_model.add(Dense(8,activation='relu'))\n",
    "base_model.add(Dense(3,activation='softmax'))\n",
    "base_model.summary()\n",
    "base_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "history_base = base_model.fit(X_train, y_train ,epochs=20, validation_split=0.2)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred1 = base_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred1, axis=1)\n",
    "\n",
    "# Print f1, precision, and recall scores\n",
    "loss, accuracy = base_model.evaluate(X_test, y_test)\n",
    "print(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))\n",
    "\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "print('Precision : ', precision_score(y_test, y_pred , average=\"macro\"))\n",
    "print('Recall : ',recall_score(y_test, y_pred , average=\"macro\"))\n",
    "print('F1-Score : ',f1_score(y_test, y_pred , average=\"macro\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b17924a",
   "metadata": {
    "id": "34232104"
   },
   "source": [
    "### Model 1: CNN - Random embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4737516c",
   "metadata": {
    "id": "4737516c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 32, 300)      6000000     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 32, 300, 1)   0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 30, 1, 100)   90100       ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 29, 1, 100)   120100      ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 28, 1, 100)   150100      ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 1, 1, 100)    0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3, 1, 100)    0           ['max_pooling2d[0][0]',          \n",
      "                                                                  'max_pooling2d_1[0][0]',        \n",
      "                                                                  'max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 300)          0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 300)          0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3)            903         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,361,203\n",
      "Trainable params: 6,361,203\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "502/502 [==============================] - 35s 68ms/step - loss: 0.7635 - accuracy: 0.7819 - val_loss: 0.5022 - val_accuracy: 0.8415\n",
      "Epoch 2/20\n",
      "502/502 [==============================] - 35s 70ms/step - loss: 0.5084 - accuracy: 0.8833 - val_loss: 0.4998 - val_accuracy: 0.8655\n",
      "Epoch 3/20\n",
      "502/502 [==============================] - 35s 70ms/step - loss: 0.4804 - accuracy: 0.9007 - val_loss: 0.4593 - val_accuracy: 0.8756\n",
      "Epoch 4/20\n",
      "502/502 [==============================] - 35s 70ms/step - loss: 0.4613 - accuracy: 0.9113 - val_loss: 0.4756 - val_accuracy: 0.8700\n",
      "Epoch 5/20\n",
      "502/502 [==============================] - 36s 73ms/step - loss: 0.4546 - accuracy: 0.9167 - val_loss: 0.4831 - val_accuracy: 0.8683\n",
      "Epoch 6/20\n",
      "502/502 [==============================] - 35s 69ms/step - loss: 0.4480 - accuracy: 0.9207 - val_loss: 0.5477 - val_accuracy: 0.8655\n",
      "Epoch 7/20\n",
      "502/502 [==============================] - 35s 70ms/step - loss: 0.4509 - accuracy: 0.9219 - val_loss: 0.5383 - val_accuracy: 0.8711\n",
      "Epoch 8/20\n",
      "502/502 [==============================] - 37s 73ms/step - loss: 0.4471 - accuracy: 0.9223 - val_loss: 0.5882 - val_accuracy: 0.8583\n",
      "Epoch 9/20\n",
      "502/502 [==============================] - 36s 71ms/step - loss: 0.4251 - accuracy: 0.9243 - val_loss: 0.5722 - val_accuracy: 0.8588\n",
      "Epoch 10/20\n",
      "502/502 [==============================] - 35s 69ms/step - loss: 0.4377 - accuracy: 0.9245 - val_loss: 0.5163 - val_accuracy: 0.8639\n",
      "Epoch 11/20\n",
      "502/502 [==============================] - 35s 69ms/step - loss: 0.4281 - accuracy: 0.9261 - val_loss: 0.7590 - val_accuracy: 0.8504\n",
      "Epoch 12/20\n",
      "502/502 [==============================] - 35s 71ms/step - loss: 0.4338 - accuracy: 0.9248 - val_loss: 0.5760 - val_accuracy: 0.8555\n",
      "Epoch 13/20\n",
      "502/502 [==============================] - 35s 70ms/step - loss: 0.4264 - accuracy: 0.9259 - val_loss: 0.5630 - val_accuracy: 0.8549\n",
      "Epoch 14/20\n",
      "502/502 [==============================] - 35s 69ms/step - loss: 0.4288 - accuracy: 0.9259 - val_loss: 0.8100 - val_accuracy: 0.8560\n",
      "Epoch 15/20\n",
      "502/502 [==============================] - 35s 70ms/step - loss: 0.4238 - accuracy: 0.9262 - val_loss: 0.9067 - val_accuracy: 0.8583\n",
      "Epoch 16/20\n",
      "502/502 [==============================] - 35s 70ms/step - loss: 0.4311 - accuracy: 0.9255 - val_loss: 0.7628 - val_accuracy: 0.8543\n",
      "Epoch 17/20\n",
      "502/502 [==============================] - 35s 70ms/step - loss: 0.4220 - accuracy: 0.9260 - val_loss: 0.7864 - val_accuracy: 0.8521\n",
      "Epoch 18/20\n",
      "502/502 [==============================] - 35s 70ms/step - loss: 0.4171 - accuracy: 0.9259 - val_loss: 0.8751 - val_accuracy: 0.8521\n",
      "Epoch 19/20\n",
      "502/502 [==============================] - 35s 70ms/step - loss: 0.4317 - accuracy: 0.9264 - val_loss: 0.7392 - val_accuracy: 0.8482\n",
      "Epoch 20/20\n",
      "502/502 [==============================] - 35s 70ms/step - loss: 0.4199 - accuracy: 0.9260 - val_loss: 0.8617 - val_accuracy: 0.8359\n",
      "62/62 [==============================] - 0s 6ms/step\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.8362 - accuracy: 0.8361\n",
      "Test: accuracy = 0.836107  ;  loss = 0.836214\n",
      "Precision :  0.5136654390385734\n",
      "Recall :  0.5258200148210966\n",
      "F1-Score :  0.518923181612938\n",
      "Precision :  0.7817055555669595\n",
      "Recall :  0.8361069087241553\n",
      "F1-Score :  0.807436059643236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300 # Kim uses 300 here\n",
    "num_filters = 100\n",
    "\n",
    "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "\n",
    "# use a random embedding for the text\n",
    "embedding_layer = Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
    "\n",
    "reshape = Reshape((sequence_length, embedding_dim, 1))(embedding_layer)\n",
    "\n",
    "# Note the relu activation which Kim specifically mentions\n",
    "# He also uses an l2 constraint of 3\n",
    "# Also, note that the convolution window acts on the whole 200 dimensions - that's important\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "\n",
    "# perform max pooling on each of the convoluations\n",
    "maxpool_0 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "# concat and flatten\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "\n",
    "# do dropout and predict\n",
    "dropout = Dropout(0.5)(flatten)\n",
    "output = Dense(units=3, activation='softmax')(dropout)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 32 # Kim uses 50 here, I have a slightly smaller sample size than num\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=batch_size, verbose=1, validation_split=0.1, shuffle=True)\n",
    "y_pred1  = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred1, axis=1)\n",
    "\n",
    "# Print f1, precision, and recall scores\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))\n",
    "\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "print('Precision : ', precision_score(y_test, y_pred , average=\"macro\"))\n",
    "print('Recall : ',recall_score(y_test, y_pred , average=\"macro\"))\n",
    "print('F1-Score : ',f1_score(y_test, y_pred , average=\"macro\"))\n",
    "\n",
    "print('Precision : ', precision_score(y_test, y_pred , average=\"weighted\"))\n",
    "print('Recall : ',recall_score(y_test, y_pred , average=\"weighted\"))\n",
    "print('F1-Score : ',f1_score(y_test, y_pred , average=\"weighted\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9c669d",
   "metadata": {
    "id": "af9c669d"
   },
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>')\n",
    "# tokenizer.fit_on_texts(test['tweet'].values)\n",
    "# # this takes our sentences and replaces each word with an integer\n",
    "# X = tokenizer.texts_to_sequences(test['tweet'].values)\n",
    "# # we then pad the sequences so they're all the same length (sequence_length)\n",
    "# X = pad_sequences(X, sequence_length)\n",
    "\n",
    "# predictions = model.predict(X)\n",
    "# y_prob = model.predict(X)\n",
    "# y_classes = y_prob.argmax(axis=-1) \n",
    "# test['label'] = y_classes\n",
    "# test.to_csv(\"D:/cnn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0451152",
   "metadata": {
    "id": "d0451152"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4b3f14",
   "metadata": {
    "id": "8d4b3f14"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15159c5",
   "metadata": {
    "id": "b15159c5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df87512e",
   "metadata": {
    "id": "df87512e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f7af7a7",
   "metadata": {
    "id": "aafecc61"
   },
   "source": [
    "### Model 2: Static word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fc4f3e6",
   "metadata": {
    "id": "1fc4f3e6",
    "outputId": "a2f24bef-b1a6-4512-fb22-f7b5cc03fc17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1917494 word vectors.\n",
      "Found 32538 unique tokens.\n",
      "20001\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join('D:/glove/', 'glove.42B.300d.txt'),encoding='utf-8') #glove.42B.300d.txt\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "embedding_dim = 300 # Kim uses 300 here\n",
    "num_filters = 100\n",
    "num_words = min(max_features, len(word_index)) + 1\n",
    "print(num_words)\n",
    "\n",
    "# first create a matrix of zeros, this is our embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "# for each word in out tokenizer lets try to find that work in our w2v model\n",
    "for word, i in word_index.items():\n",
    "    if i > max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # we found the word - add that words vector to the matrix\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # doesn't exist, assign a random vector\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b593f5",
   "metadata": {
    "id": "67b593f5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c92c46",
   "metadata": {
    "id": "84c92c46"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73b8f965",
   "metadata": {
    "id": "73b8f965"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 32, 300)      6000300     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 32, 300, 1)   0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 30, 1, 100)   90100       ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 29, 1, 100)   120100      ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 28, 1, 100)   150100      ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 1, 1, 100)    0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3, 1, 100)    0           ['max_pooling2d[0][0]',          \n",
      "                                                                  'max_pooling2d_1[0][0]',        \n",
      "                                                                  'max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 300)          0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 300)          0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3)            903         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,361,503\n",
      "Trainable params: 361,203\n",
      "Non-trainable params: 6,000,300\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "447/447 [==============================] - 9s 18ms/step - loss: 1.1148 - accuracy: 0.7777 - val_loss: 0.9297 - val_accuracy: 0.7840\n",
      "Epoch 2/20\n",
      "447/447 [==============================] - 8s 18ms/step - loss: 0.9589 - accuracy: 0.7984 - val_loss: 1.1085 - val_accuracy: 0.8162\n",
      "Epoch 3/20\n",
      "447/447 [==============================] - 9s 19ms/step - loss: 0.9283 - accuracy: 0.8157 - val_loss: 0.8274 - val_accuracy: 0.8692\n",
      "Epoch 4/20\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 0.8401 - accuracy: 0.8292 - val_loss: 0.8778 - val_accuracy: 0.8689\n",
      "Epoch 5/20\n",
      "447/447 [==============================] - 8s 19ms/step - loss: 0.7950 - accuracy: 0.8364 - val_loss: 0.7741 - val_accuracy: 0.8621\n",
      "Epoch 6/20\n",
      "447/447 [==============================] - 9s 19ms/step - loss: 0.7634 - accuracy: 0.8221 - val_loss: 0.6702 - val_accuracy: 0.8493\n",
      "Epoch 7/20\n",
      "447/447 [==============================] - 9s 19ms/step - loss: 0.7131 - accuracy: 0.8247 - val_loss: 0.7145 - val_accuracy: 0.7983\n",
      "Epoch 8/20\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 0.7051 - accuracy: 0.8339 - val_loss: 0.6618 - val_accuracy: 0.8263\n",
      "Epoch 9/20\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 0.6936 - accuracy: 0.8027 - val_loss: 0.6593 - val_accuracy: 0.7929\n",
      "Epoch 10/20\n",
      "447/447 [==============================] - 8s 18ms/step - loss: 0.6929 - accuracy: 0.8066 - val_loss: 0.6668 - val_accuracy: 0.8302\n",
      "Epoch 11/20\n",
      "447/447 [==============================] - 9s 19ms/step - loss: 0.6721 - accuracy: 0.8176 - val_loss: 0.6460 - val_accuracy: 0.8179\n",
      "Epoch 12/20\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 0.6668 - accuracy: 0.8185 - val_loss: 0.6785 - val_accuracy: 0.7969\n",
      "Epoch 13/20\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 0.6709 - accuracy: 0.8179 - val_loss: 0.6213 - val_accuracy: 0.8252\n",
      "Epoch 14/20\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 0.6652 - accuracy: 0.8239 - val_loss: 0.6235 - val_accuracy: 0.8061\n",
      "Epoch 15/20\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 0.6702 - accuracy: 0.8244 - val_loss: 0.6849 - val_accuracy: 0.8649\n",
      "Epoch 16/20\n",
      "447/447 [==============================] - 10s 21ms/step - loss: 0.6726 - accuracy: 0.8204 - val_loss: 0.7154 - val_accuracy: 0.8599\n",
      "Epoch 17/20\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 0.6759 - accuracy: 0.7981 - val_loss: 0.5662 - val_accuracy: 0.7840\n",
      "Epoch 18/20\n",
      "447/447 [==============================] - 8s 19ms/step - loss: 0.6537 - accuracy: 0.8029 - val_loss: 0.5899 - val_accuracy: 0.8563\n",
      "Epoch 19/20\n",
      "447/447 [==============================] - 9s 20ms/step - loss: 0.6602 - accuracy: 0.8036 - val_loss: 0.6363 - val_accuracy: 0.8666\n",
      "Epoch 20/20\n",
      "447/447 [==============================] - 9s 21ms/step - loss: 0.6661 - accuracy: 0.7967 - val_loss: 0.6561 - val_accuracy: 0.8106\n",
      "62/62 [==============================] - 0s 6ms/step\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.6631 - accuracy: 0.7978\n",
      "Test: accuracy = 0.797781  ;  loss = 0.663124\n",
      "Precision :  0.5235500825494919\n",
      "Recall :  0.4113558381269553\n",
      "F1-Score :  0.41943000304261596\n",
      "Precision :  0.7523019162219988\n",
      "Recall :  0.7977811396873424\n",
      "F1-Score :  0.7427455323794869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "inputs_2 = Input(shape=(sequence_length,), dtype='int32')\n",
    "\n",
    "# note the `trainable=False`, later we will make this layer trainable\n",
    "embedding_layer_2 = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=sequence_length,\n",
    "                            trainable=False)(inputs_2)\n",
    "\n",
    "reshape_2 = Reshape((sequence_length, embedding_dim, 1))(embedding_layer_2)\n",
    "\n",
    "conv_0_2 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_2)\n",
    "conv_1_2 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_2)\n",
    "conv_2_2 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_2)\n",
    "\n",
    "maxpool_0_2 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0_2)\n",
    "maxpool_1_2 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1_2)\n",
    "maxpool_2_2 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2_2)\n",
    "\n",
    "concatenated_tensor_2 = Concatenate(axis=1)([maxpool_0_2, maxpool_1_2, maxpool_2_2])\n",
    "flatten_2 = Flatten()(concatenated_tensor_2)\n",
    "\n",
    "dropout_2 = Dropout(0.5)(flatten_2)\n",
    "output_2 = Dense(units=3, activation='softmax')(dropout_2)\n",
    "\n",
    "model_2 = Model(inputs=inputs_2, outputs=output_2)\n",
    "model_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_2.summary())\n",
    "\n",
    "batch_size = 32\n",
    "history_2 = model_2.fit(X_train, y_train, epochs=20, batch_size=batch_size, verbose=1, validation_split=0.2)\n",
    "y_pred1  = model_2.predict(X_test)\n",
    "y_pred = np.argmax(y_pred1, axis=1)\n",
    "\n",
    "# Print f1, precision, and recall scores\n",
    "loss, accuracy = model_2.evaluate(X_test, y_test)\n",
    "print(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))\n",
    "\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "print('Precision : ', precision_score(y_test, y_pred , average=\"macro\"))\n",
    "print('Recall : ',recall_score(y_test, y_pred , average=\"macro\"))\n",
    "print('F1-Score : ',f1_score(y_test, y_pred , average=\"macro\"))\n",
    "\n",
    "\n",
    "print('Precision : ', precision_score(y_test, y_pred , average=\"weighted\"))\n",
    "print('Recall : ',recall_score(y_test, y_pred , average=\"weighted\"))\n",
    "print('F1-Score : ',f1_score(y_test, y_pred , average=\"weighted\"))\n",
    "\n",
    "# predictions = model_2.predict(x_test)\n",
    "# y_prob = model_2.predict(x_test)\n",
    "# y_classes = y_prob.argmax(axis=-1) \n",
    "# test['label'] = y_classes\n",
    "# test.to_csv(\"D:/cnn1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6f8ad8",
   "metadata": {
    "id": "6f6f8ad8"
   },
   "outputs": [],
   "source": [
    "# plt.plot(history_2.history['accuracy'])\n",
    "# plt.plot(history_2.history['val_accuracy'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'validation'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(history_2.history['loss'])\n",
    "# plt.plot(history_2.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'validation'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408c628f",
   "metadata": {
    "id": "04329546"
   },
   "source": [
    "### Model 3: w2v with trainable embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d576313b",
   "metadata": {
    "id": "d576313b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 32, 300)      6000300     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 32, 300, 1)   0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 30, 1, 100)   90100       ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 29, 1, 100)   120100      ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 28, 1, 100)   150100      ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 1, 1, 100)    0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3, 1, 100)    0           ['max_pooling2d[0][0]',          \n",
      "                                                                  'max_pooling2d_1[0][0]',        \n",
      "                                                                  'max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 300)          0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 300)          0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3)            903         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,361,503\n",
      "Trainable params: 6,361,503\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "447/447 [==============================] - 31s 68ms/step - loss: 0.9965 - accuracy: 0.8073 - val_loss: 0.8036 - val_accuracy: 0.8423\n",
      "Epoch 2/20\n",
      "447/447 [==============================] - 32s 72ms/step - loss: 0.7777 - accuracy: 0.8754 - val_loss: 1.0340 - val_accuracy: 0.8260\n",
      "Epoch 3/20\n",
      "447/447 [==============================] - 30s 67ms/step - loss: 0.7382 - accuracy: 0.8887 - val_loss: 0.8261 - val_accuracy: 0.8700\n",
      "Epoch 4/20\n",
      "447/447 [==============================] - 30s 68ms/step - loss: 0.6997 - accuracy: 0.8959 - val_loss: 0.6104 - val_accuracy: 0.8949\n",
      "Epoch 5/20\n",
      "447/447 [==============================] - 30s 67ms/step - loss: 0.6687 - accuracy: 0.8995 - val_loss: 0.6917 - val_accuracy: 0.8977\n",
      "Epoch 6/20\n",
      "447/447 [==============================] - 30s 67ms/step - loss: 0.6516 - accuracy: 0.9033 - val_loss: 0.7124 - val_accuracy: 0.8955\n",
      "Epoch 7/20\n",
      "447/447 [==============================] - 31s 69ms/step - loss: 0.6515 - accuracy: 0.9065 - val_loss: 0.6887 - val_accuracy: 0.8952\n",
      "Epoch 8/20\n",
      "447/447 [==============================] - 30s 68ms/step - loss: 0.6367 - accuracy: 0.9079 - val_loss: 0.6146 - val_accuracy: 0.8983\n",
      "Epoch 9/20\n",
      "447/447 [==============================] - 30s 68ms/step - loss: 0.6345 - accuracy: 0.9100 - val_loss: 1.0235 - val_accuracy: 0.8451\n",
      "Epoch 10/20\n",
      "447/447 [==============================] - 30s 68ms/step - loss: 0.6752 - accuracy: 0.9099 - val_loss: 0.7210 - val_accuracy: 0.8725\n",
      "Epoch 11/20\n",
      "447/447 [==============================] - 31s 69ms/step - loss: 0.6150 - accuracy: 0.9131 - val_loss: 0.8045 - val_accuracy: 0.8899\n",
      "Epoch 12/20\n",
      "447/447 [==============================] - 31s 69ms/step - loss: 0.6140 - accuracy: 0.9158 - val_loss: 0.7319 - val_accuracy: 0.8941\n",
      "Epoch 13/20\n",
      "447/447 [==============================] - 31s 68ms/step - loss: 0.6026 - accuracy: 0.9164 - val_loss: 0.7425 - val_accuracy: 0.8865\n",
      "Epoch 14/20\n",
      "447/447 [==============================] - 31s 69ms/step - loss: 0.6097 - accuracy: 0.9174 - val_loss: 0.6415 - val_accuracy: 0.8904\n",
      "Epoch 15/20\n",
      "447/447 [==============================] - 32s 71ms/step - loss: 0.6043 - accuracy: 0.9182 - val_loss: 0.5576 - val_accuracy: 0.8930\n",
      "Epoch 16/20\n",
      "447/447 [==============================] - 32s 73ms/step - loss: 0.5890 - accuracy: 0.9194 - val_loss: 0.6798 - val_accuracy: 0.8904\n",
      "Epoch 17/20\n",
      "447/447 [==============================] - 31s 69ms/step - loss: 0.5942 - accuracy: 0.9199 - val_loss: 0.6804 - val_accuracy: 0.8893\n",
      "Epoch 18/20\n",
      "447/447 [==============================] - 31s 69ms/step - loss: 0.5755 - accuracy: 0.9224 - val_loss: 0.6497 - val_accuracy: 0.8904\n",
      "Epoch 19/20\n",
      "447/447 [==============================] - 31s 69ms/step - loss: 0.5788 - accuracy: 0.9218 - val_loss: 0.8791 - val_accuracy: 0.8801\n",
      "Epoch 20/20\n",
      "447/447 [==============================] - 31s 69ms/step - loss: 0.5945 - accuracy: 0.9216 - val_loss: 0.8508 - val_accuracy: 0.8795\n",
      "62/62 [==============================] - 0s 6ms/step\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.8061 - accuracy: 0.8855\n",
      "Test: accuracy = 0.885527  ;  loss = 0.806094\n",
      "Precision :  0.5827038535235256\n",
      "Recall :  0.5736297232245428\n",
      "F1-Score :  0.5760607199845413\n",
      "Precision :  0.8345752436266064\n",
      "Recall :  0.8855269793242562\n",
      "F1-Score :  0.8577232516728883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "inputs_3 = Input(shape=(sequence_length,), dtype='int32')\n",
    "embedding_layer_3 = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=sequence_length,\n",
    "                            trainable=True)(inputs_3)\n",
    "\n",
    "reshape_3 = Reshape((sequence_length, embedding_dim, 1))(embedding_layer_3)\n",
    "\n",
    "# note the relu activation\n",
    "conv_0_3 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)\n",
    "conv_1_3 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)\n",
    "conv_2_3 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)\n",
    "\n",
    "maxpool_0_3 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0_3)\n",
    "maxpool_1_3 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1_3)\n",
    "maxpool_2_3 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2_3)\n",
    "\n",
    "concatenated_tensor_3 = Concatenate(axis=1)([maxpool_0_3, maxpool_1_3, maxpool_2_3])\n",
    "flatten_3 = Flatten()(concatenated_tensor_3)\n",
    "\n",
    "dropout_3 = Dropout(0.5)(flatten_3)\n",
    "output_3 = Dense(units=3, activation='softmax')(dropout_3)\n",
    "\n",
    "model_3 = Model(inputs=inputs_3, outputs=output_3)\n",
    "model_3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_3.summary())\n",
    "\n",
    "batch_size = 32\n",
    "history_3 = model_3.fit(X_train, y_train, epochs=20, batch_size=batch_size, verbose=1, validation_split=0.2)\n",
    "y_pred1  = model_3.predict(X_test)\n",
    "y_pred = np.argmax(y_pred1, axis=1)\n",
    "\n",
    "# Print f1, precision, and recall scores\n",
    "loss, accuracy = model_3.evaluate(X_test, y_test)\n",
    "print(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))\n",
    "\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "print('Precision : ', precision_score(y_test, y_pred , average=\"macro\"))\n",
    "print('Recall : ',recall_score(y_test, y_pred , average=\"macro\"))\n",
    "print('F1-Score : ',f1_score(y_test, y_pred , average=\"macro\"))\n",
    "\n",
    "print('Precision : ', precision_score(y_test, y_pred , average=\"weighted\"))\n",
    "print('Recall : ',recall_score(y_test, y_pred , average=\"weighted\"))\n",
    "print('F1-Score : ',f1_score(y_test, y_pred , average=\"weighted\"))\n",
    "\n",
    "# predictions = model_3.predict(x_test)\n",
    "# y_prob = model_3.predict(x_test)\n",
    "# y_classes = y_prob.argmax(axis=-1) \n",
    "# test['label'] = y_classes\n",
    "# test.to_csv(\"D:/cnn2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477b38f8",
   "metadata": {
    "id": "477b38f8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01b746a",
   "metadata": {
    "id": "b01b746a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe71e41",
   "metadata": {
    "id": "efe71e41"
   },
   "outputs": [],
   "source": [
    "# plt.plot(history_3.history['accuracy'])\n",
    "# plt.plot(history_3.history['val_accuracy'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'validation'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(history_3.history['loss'])\n",
    "# plt.plot(history_3.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'validation'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e1b85c",
   "metadata": {
    "id": "34e1b85c"
   },
   "outputs": [],
   "source": [
    "print(\"CNN random       : \" + str(accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))))\n",
    "print(\"CNN static       : \" + str(accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_2)))))\n",
    "print(\"CNN trainable    : \" + str(accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat_3)))))\n",
    "# LR = 79\n",
    "# NB = 75\n",
    "# DT = 73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bae988ce",
   "metadata": {
    "id": "bae988ce"
   },
   "outputs": [],
   "source": [
    "### tuning the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8c6ec8d",
   "metadata": {
    "id": "f8c6ec8d",
    "outputId": "e752da92-7b42-4f20-baaf-440e2dd53c26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32538 unique tokens.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 32, 128)           2560128   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32, 128)           0         \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 1032      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 27        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,561,187\n",
      "Trainable params: 2,561,187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "447/447 [==============================] - 13s 28ms/step - loss: 0.7597 - accuracy: 0.7727 - val_loss: 0.5689 - val_accuracy: 0.7716\n",
      "Epoch 2/20\n",
      "447/447 [==============================] - 11s 26ms/step - loss: 0.5576 - accuracy: 0.8121 - val_loss: 0.4548 - val_accuracy: 0.8490\n",
      "Epoch 3/20\n",
      "447/447 [==============================] - 12s 27ms/step - loss: 0.4815 - accuracy: 0.8335 - val_loss: 0.4275 - val_accuracy: 0.8627\n",
      "Epoch 4/20\n",
      "447/447 [==============================] - 11s 25ms/step - loss: 0.4477 - accuracy: 0.8402 - val_loss: 0.4137 - val_accuracy: 0.8638\n",
      "Epoch 5/20\n",
      "447/447 [==============================] - 12s 26ms/step - loss: 0.4271 - accuracy: 0.8452 - val_loss: 0.3997 - val_accuracy: 0.8731\n",
      "Epoch 6/20\n",
      "447/447 [==============================] - 12s 26ms/step - loss: 0.4130 - accuracy: 0.8476 - val_loss: 0.3924 - val_accuracy: 0.8736\n",
      "Epoch 7/20\n",
      "447/447 [==============================] - 13s 28ms/step - loss: 0.4005 - accuracy: 0.8483 - val_loss: 0.3890 - val_accuracy: 0.8734\n",
      "Epoch 8/20\n",
      "447/447 [==============================] - 12s 27ms/step - loss: 0.3939 - accuracy: 0.8480 - val_loss: 0.3918 - val_accuracy: 0.8728\n",
      "Epoch 9/20\n",
      "447/447 [==============================] - 12s 26ms/step - loss: 0.3783 - accuracy: 0.8523 - val_loss: 0.3903 - val_accuracy: 0.8722\n",
      "Epoch 10/20\n",
      "447/447 [==============================] - 12s 27ms/step - loss: 0.3778 - accuracy: 0.8511 - val_loss: 0.4027 - val_accuracy: 0.8658\n",
      "Epoch 11/20\n",
      "447/447 [==============================] - 12s 27ms/step - loss: 0.3729 - accuracy: 0.8497 - val_loss: 0.3921 - val_accuracy: 0.8720\n",
      "Epoch 12/20\n",
      "447/447 [==============================] - 13s 28ms/step - loss: 0.3603 - accuracy: 0.8529 - val_loss: 0.3968 - val_accuracy: 0.8700\n",
      "Epoch 13/20\n",
      "447/447 [==============================] - 12s 26ms/step - loss: 0.3545 - accuracy: 0.8529 - val_loss: 0.3978 - val_accuracy: 0.8703\n",
      "Epoch 14/20\n",
      "447/447 [==============================] - 12s 27ms/step - loss: 0.3484 - accuracy: 0.8530 - val_loss: 0.4068 - val_accuracy: 0.8675\n",
      "Epoch 15/20\n",
      "447/447 [==============================] - 12s 27ms/step - loss: 0.3482 - accuracy: 0.8538 - val_loss: 0.4179 - val_accuracy: 0.8663\n",
      "Epoch 16/20\n",
      "447/447 [==============================] - 12s 26ms/step - loss: 0.3478 - accuracy: 0.8518 - val_loss: 0.4176 - val_accuracy: 0.8644\n",
      "Epoch 17/20\n",
      "447/447 [==============================] - 12s 27ms/step - loss: 0.3403 - accuracy: 0.8513 - val_loss: 0.4241 - val_accuracy: 0.8672\n",
      "Epoch 18/20\n",
      "447/447 [==============================] - 12s 27ms/step - loss: 0.3409 - accuracy: 0.8503 - val_loss: 0.4531 - val_accuracy: 0.8624\n",
      "Epoch 19/20\n",
      "447/447 [==============================] - 12s 26ms/step - loss: 0.3262 - accuracy: 0.8582 - val_loss: 0.4827 - val_accuracy: 0.8596\n",
      "Epoch 20/20\n",
      "447/447 [==============================] - 12s 26ms/step - loss: 0.3235 - accuracy: 0.8560 - val_loss: 0.4597 - val_accuracy: 0.8599\n",
      "62/62 [==============================] - 0s 2ms/step\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4720 - accuracy: 0.8654\n",
      "Test: accuracy = 0.865356  ;  loss = 0.471996\n",
      "Precision :  0.5363895604736223\n",
      "Recall :  0.5426943411761943\n",
      "F1-Score :  0.538829172102315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Removing overfitiing\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "embedding_dim = 300 # Kim uses 300 here\n",
    "num_filters = 100\n",
    "num_words = min(max_features, len(word_index)) + 1\n",
    "regularise = tf.keras.regularizers.l2(0.001)\n",
    "\n",
    "model_r = Sequential()\n",
    "model_r.add(Embedding(num_words,128,input_length=X_train.shape[1]))\n",
    "model_r.add(Dropout(0.5))\n",
    "model_r.add(GlobalAveragePooling1D())\n",
    "model_r.add(Dense(8,activation='relu',kernel_regularizer=regularise))\n",
    "model_r.add(Dropout(0.5))\n",
    "model_r.add(Dense(3,activation='softmax'))\n",
    "model_r.summary()\n",
    "\n",
    "#Compiling the model\n",
    "model_r.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "history_r = model_r.fit(X_train, y_train ,epochs=20, validation_split=0.2)\n",
    "\n",
    "y_pred1  = model_r.predict(X_test)\n",
    "y_pred = np.argmax(y_pred1, axis=1)\n",
    "\n",
    "# Print f1, precision, and recall scores\n",
    "loss, accuracy = model_r.evaluate(X_test, y_test)\n",
    "print(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))\n",
    "\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "print('Precision : ', precision_score(y_test, y_pred , average=\"macro\"))\n",
    "print('Recall : ',recall_score(y_test, y_pred , average=\"macro\"))\n",
    "print('F1-Score : ',f1_score(y_test, y_pred , average=\"macro\"))\n",
    "\n",
    "print('Precision : ', precision_score(y_test, y_pred , average=\"weighted\"))\n",
    "print('Recall : ',recall_score(y_test, y_pred , average=\"weighted\"))\n",
    "print('F1-Score : ',f1_score(y_test, y_pred , average=\"weighted\"))\n",
    "\n",
    "# predictions = model_r.predict(x_test)\n",
    "# y_prob = model_r.predict(x_test)\n",
    "# y_classes = y_prob.argmax(axis=-1) \n",
    "# test['label'] = y_classes\n",
    "# test.to_csv(\"D:/cnn4.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cd5d96",
   "metadata": {
    "id": "6922baf3"
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18790df7",
   "metadata": {
    "id": "18790df7",
    "outputId": "1c0c502d-c7ea-426c-a887-81365a68cb44",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "447/447 [==============================] - 15s 28ms/step - loss: 0.5294 - accuracy: 0.8309 - val_loss: 0.3803 - val_accuracy: 0.8745\n",
      "Epoch 2/20\n",
      "447/447 [==============================] - 12s 28ms/step - loss: 0.3218 - accuracy: 0.9026 - val_loss: 0.3376 - val_accuracy: 0.8851\n",
      "Epoch 3/20\n",
      "447/447 [==============================] - 13s 29ms/step - loss: 0.2336 - accuracy: 0.9231 - val_loss: 0.3626 - val_accuracy: 0.8834\n",
      "Epoch 4/20\n",
      "447/447 [==============================] - 12s 28ms/step - loss: 0.1879 - accuracy: 0.9345 - val_loss: 0.3841 - val_accuracy: 0.8834\n",
      "Epoch 5/20\n",
      "447/447 [==============================] - 12s 27ms/step - loss: 0.1506 - accuracy: 0.9497 - val_loss: 0.4453 - val_accuracy: 0.8826\n",
      "62/62 [==============================] - 1s 4ms/step\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.4180 - accuracy: 0.8891\n",
      "Test: accuracy = 0.889057  ;  loss = 0.418050\n",
      "Precision :  0.7130921029285266\n",
      "Recall :  0.6715123702608058\n",
      "F1-Score :  0.6862793335191553\n",
      "Precision :  0.8788099876595281\n",
      "Recall :  0.8890569843671206\n",
      "F1-Score :  0.8829083777899176\n"
     ]
    }
   ],
   "source": [
    "#Building the model\n",
    "from keras.constraints import max_norm\n",
    "regularise = tf.keras.regularizers.l2(0.001)\n",
    "model1 = Sequential([\n",
    "    Embedding(num_words,128,input_length=X_train.shape[1]),\n",
    "    Dropout(0.5),\n",
    "    LSTM(32,kernel_constraint=max_norm(3)),\n",
    "    Dense(32,activation='relu',kernel_regularizer=regularise),\n",
    "    Dropout(0.5),\n",
    "    Dense(3,activation='softmax')\n",
    "])\n",
    "#Compiling the model\n",
    "model1.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "#Fitting the model\n",
    "history1 =  model1.fit(X_train, y_train ,epochs=20, validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',patience=3)])\n",
    "y_pred1  = model1.predict(X_test)\n",
    "y_pred = np.argmax(y_pred1, axis=1)\n",
    "\n",
    "# Print f1, precision, and recall scores\n",
    "loss, accuracy = model1.evaluate(X_test, y_test)\n",
    "print(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))\n",
    "\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "print('Precision : ', precision_score(y_test, y_pred , average=\"macro\"))\n",
    "print('Recall : ',recall_score(y_test, y_pred , average=\"macro\"))\n",
    "print('F1-Score : ',f1_score(y_test, y_pred , average=\"macro\"))\n",
    "\n",
    "print('Precision : ', precision_score(y_test, y_pred , average=\"weighted\"))\n",
    "print('Recall : ',recall_score(y_test, y_pred , average=\"weighted\"))\n",
    "print('F1-Score : ',f1_score(y_test, y_pred , average=\"weighted\"))\n",
    "# predictions = model1.predict(x_test)\n",
    "# y_prob = model1.predict(x_test)\n",
    "# y_classes = y_prob.argmax(axis=-1) \n",
    "# test['label'] = y_classes\n",
    "# test.to_csv(\"D:/cnn5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a44c2d2",
   "metadata": {
    "id": "1a44c2d2"
   },
   "outputs": [],
   "source": [
    "#tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "948e4c15",
   "metadata": {
    "id": "948e4c15",
    "outputId": "4e1d891d-64f6-42b7-ec31-7e93997c4e99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "447/447 [==============================] - 45s 84ms/step - loss: 0.4556 - accuracy: 0.8503 - val_loss: 0.3168 - val_accuracy: 0.8902\n",
      "Epoch 2/20\n",
      "447/447 [==============================] - 34s 75ms/step - loss: 0.2638 - accuracy: 0.9141 - val_loss: 0.3399 - val_accuracy: 0.8829\n",
      "Epoch 3/20\n",
      "447/447 [==============================] - 33s 75ms/step - loss: 0.1773 - accuracy: 0.9425 - val_loss: 0.3675 - val_accuracy: 0.8762\n",
      "Epoch 4/20\n",
      "447/447 [==============================] - 33s 73ms/step - loss: 0.1187 - accuracy: 0.9632 - val_loss: 0.4348 - val_accuracy: 0.8792\n",
      "62/62 [==============================] - 3s 17ms/step\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.4474 - accuracy: 0.8684\n",
      "Test: accuracy = 0.868381  ;  loss = 0.447356\n",
      "Precision :  0.6815146141934361\n",
      "Recall :  0.6440775683085872\n",
      "F1-Score :  0.6560588289061624\n",
      "Precision :  0.8560251067608488\n",
      "Recall :  0.8683812405446294\n",
      "F1-Score :  0.860889678499515\n"
     ]
    }
   ],
   "source": [
    "#Building the model\n",
    "model2 = Sequential([\n",
    "    Embedding(num_words,128,input_length=X_train.shape[1]),\n",
    "    Bidirectional(LSTM(128,return_sequences=True)),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dense(32,activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3,activation='softmax')\n",
    "])\n",
    "#Compiling the model\n",
    "model2.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "#Fitting the model\n",
    "history2 =  model2.fit(X_train, y_train ,epochs=20, validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',patience=3)])\n",
    "y_pred1  = model2.predict(X_test)\n",
    "y_pred = np.argmax(y_pred1, axis=1)\n",
    "\n",
    "# Print f1, precision, and recall scores\n",
    "loss, accuracy = model2.evaluate(X_test, y_test)\n",
    "print(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))\n",
    "\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "print('Precision : ', precision_score(y_test, y_pred , average=\"macro\"))\n",
    "print('Recall : ',recall_score(y_test, y_pred , average=\"macro\"))\n",
    "print('F1-Score : ',f1_score(y_test, y_pred , average=\"macro\"))\n",
    "\n",
    "\n",
    "print('Precision : ', precision_score(y_test, y_pred , average=\"weighted\"))\n",
    "print('Recall : ',recall_score(y_test, y_pred , average=\"weighted\"))\n",
    "print('F1-Score : ',f1_score(y_test, y_pred , average=\"weighted\"))\n",
    "\n",
    "# predictions = model2.predict(x_test)\n",
    "# y_prob = model2.predict(x_test)\n",
    "# y_classes = y_prob.argmax(axis=-1) \n",
    "# test['label'] = y_classes\n",
    "# test.to_csv(\"D:/cnn6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba8662",
   "metadata": {
    "id": "68ba8662"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
